
# Persistent Volumes â€“ Ä†wiczenia

## Przepisz Ä‡wiczenie z Wolumeny w Pod â€“ Ä†wiczenia tak by zostaÅ‚y wykorzystane wolumeny za pomocÄ…:

---

### UÅ¼yj PersistentVolume i PersistentVolumeClaim aby zcacheowaÄ‡ repozytorium git za pomocÄ… InitContainer

tworzymy definicje PersistenceVolume
```
apiVersion: v1
kind: PersistentVolume
metadata:
  name: demo-pv
  labels:
    purpose: demo
spec:
  storageClassName: ""
  capacity:
    storage: 10Mi
  persistentVolumeReclaimPolicy: Retain
  accessModes:
  - ReadWriteOnce
  hostPath:
    path: /d/k8s_vol/persistent_volume/retain
```
dodajemy definicjÄ™ PersistentVolumeClaim ktÃ³ry pozwoli nam zamontowaÄ‡ dysk jako volumen. 
Za pomocÄ… `matchLabels` wskazujemy `label` zgody z naszym PersistenVolumne, upewniamy siÄ™ rÃ³wnieÅ¼ Å¼e `accessModes` w PersistentVolumeClaim i PersistenVolume sÄ… zgodne
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: demo-pvc
spec:
  storageClassName: ""
  selector:
    matchLabels:
      purpose: demo
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 0.5Mi
```
dodajemy definicjÄ™ pod ktÃ³ra w InitContainer wyciÄ…gnie repozytorium git i zapisze pliki na PersistentVolume, a nastÄ™pnie podmontuje i udostÄ™pni index.html przez instancjÄ™ nginx
```
apiVersion: v1
kind: Pod
metadata:
  name: web-pv-static
spec:
  initContainers:
    - image: ubuntu:latest
      name: setup
      volumeMounts:
        - name: workdir
          mountPath: /repo
      command: ["/bin/sh", "-c"]
      args:
        - apt-get update;
          apt-get install -y git;
          git clone "https://github.com/KrzyskowK/poznajKubernetes.git" /repo;
  containers:
    - image: nginx:1.17.6
      name: web
      ports:
        - containerPort: 80
      volumeMounts:
        - name: workdir
          mountPath: /usr/share/nginx/html
  volumes:
    - name: workdir
      persistentVolumeClaim:
        claimName: demo-pvc
```
wgrywamy konfiguracjÄ… na klaster:
```
> kubectl apply -f .\pv-pvc-static.yaml
persistentvolume/demo-pv created
persistentvolumeclaim/demo-pvc created
pod/web-pv-static created
```
widzimy, Å¼e jako pierwsze zostaje stworzony PVC, ktÃ³ry w momenci pojawienia siÄ™ PV zmienai status na `Bound`
```
> kubectl get pvc
NAME       STATUS   VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE
demo-pvc   Bound    demo-pv   10Mi       RWO                           13s

> kubectl get pv
NAME      CAPACITY   ACCESS MODES   RECLAIM POLICY   STATUS   CLAIM              STORAGECLASS   REASON   AGE
demo-pv   10Mi       RWO            Retain           Bound    default/demo-pvc                           11s
```
widzimy, Å¼e pod zostaÅ‚ poprawnie uruchomiony
```
> kubectl get pod web-pv-static -o wide
NAME            READY   STATUS    RESTARTS   AGE     IP           NODE             NOMINATED NODE   READINESS GATES
web-pv-static   1/1     Running   0          2m34s   10.1.1.209   docker-desktop   <none>           <none>
```
sprawdzamy, czy repo zostaÅ‚o poprawnie wyciÄ…gniete i nasz nginx serwuje teraz https://github.com/KrzyskowK/poznajKubernetes/blob/master/index.html
```
> kubectl port-forward web-pv-static 8080:80

> curl localhost:8080
<h1>Ahoj! ğŸš¢ğŸ“¦ğŸ´â€â˜ ï¸</h1>
```
sukces. Sprawdzimy teraz co siÄ™ stanie jeÅ¼eli uruchomimy nastÄ™pny pod z volume przypiÄ™tym do tego samego pvc
```
apiVersion: v1
kind: Pod
metadata:
  name: web-pv-static-2
spec:
  containers:
    - image: nginx:1.17.6
      name: web-pv-static-2
      ports:
        - containerPort: 80
      volumeMounts:
        - name: workdir
          mountPath: /usr/share/nginx/html
  volumes:
    - name: workdir
      persistentVolumeClaim:
        claimName: demo-pvc
```
widzimy Å¼e nowo dodany pod `web-pv-static-2` jest w stanie poprawnie odczytaÄ‡ index.html z istniejÄ…cego PV
```
> kubectl apply -f .\pv-pvc-static-pod-attched-after.yaml
pod/web-pv-static-2 created
> kubectl port-forward web-pv-static-2 8080:80
Forwarding from 127.0.0.1:8080 -> 80
Forwarding from [::1]:8080 -> 80

> curl localhost:8080
<h1>Ahoj! ğŸš¢ğŸ“¦ğŸ´â€â˜ ï¸</h1>
```
sprawdÅºmy co siÄ™ stanie jeÅ¼eli dodamy teraz pod ktÃ³ry bÄ™dzie prÃ³bowaÅ‚ nadpisaÄ‡ `index.html` w initContainer
```
apiVersion: v1
kind: Pod
metadata:
  name: web-pv-static-3
spec:
  initContainers:
    - image: ubuntu:latest
      name: setup
      volumeMounts:
        - name: workdir
          mountPath: /repo
      command: ["/bin/sh", "-c"]
      args:
        - echo "hello !!!" > /repo/index.html;
  containers:
    - image: nginx:1.17.6
      name: web
      ports:
        - containerPort: 80
      volumeMounts:
        - name: workdir
          mountPath: /usr/share/nginx/html
  volumes:
    - name: workdir
      persistentVolumeClaim:
        claimName: demo-pvc
```
Widzimy, Å¼e index zostaÅ‚ `index.html` zostaÅ‚ poprawnie nadpisany w PV, widzimy rÃ³wnieÅ¼, Å¼e ma to efekt dla wszyztkich pod ktÃ³re hostowaÅ‚y ten sam `index.html`
```
> kubectl apply -f pv-pvc-static-overwrite.yaml
pod/web-pv-static-3 created

> kubectl port-forward web-pv-static-3 8080:80
> curl localhost:8080
hello !!!

> kubectl port-forward web-pv-static-2 8080:80
> curl localhost:8080
hello !!!

> kubectl port-forward web-pv-static 8080:80
> curl localhost:8080
hello !!!
```
---

### UÅ¼yj PersistentVolume i PersistentVolumeClaim aby dostarczyÄ‡ plik konfiguracyjny za pomocÄ… subpath

Na poczÄ…tek sprÃ³bujmy ponownie uÅ¼yÄ‡ istniejÄ…cego PV `demo-pv`, stwÃ³rzmy jedynie nowy PersistentVolumeClaim `demo-pv-subpath`.
Widzimy jednak, Å¼e nowy PVC nie zamontuje siÄ™ automatycznie do PV
```
> kubectl get pvc
NAME               STATUS    VOLUME    CAPACITY   ACCESS MODES   STORAGECLASS   AGE
demo-pvc           Bound     demo-pv   10Mi       RWO                           104m
demo-pvc-subpath   Pending                                                      53s
```
JeÅ¼eli sprawdzimy eventy `demo-pvc-subpath` dowiemy siÄ™, Å¼e `no persistent volumes available for this claim`.
Dzieje siÄ™ tak poniewaÅ¼ jeden statyczny PV moÅ¼e byÄ‡ w danej chwili wykorzystywany przez jedno PVC. Dodatkow `demo-pvc` zostaÅ‚o stworzone z `reclaimPolicy: retain`
co oznacz Å¼e nawet po usuniÄ™ciu obecnie z-boundowaneg claima `demo-pvc` nie bÄ™dziemy mogli siÄ™ do niego podÅ‚Ä…czyÄ‡ dopÃ³ki nie zostanie on manualnie wyczyszczony
```
> kubectl describe pvc demo-pvc-subpath
Name:          demo-pvc-subpath
Namespace:     default
Status:        Pending
Volume:
Labels:        <none>
Annotations:   kubectl.kubernetes.io/last-applied-configuration:
                 {"apiVersion":"v1","kind":"PersistentVolumeClaim","metadata":{"annotations":{},"name":"demo-pvc-subpath","namespace":"default"},"spec":{"a...
Finalizers:    [kubernetes.io/pvc-protection]
Capacity:
Access Modes:
VolumeMode:    Filesystem
Events:
  ----       ------         ----              ----                         -------
  Normal     FailedBinding  3s (x8 over 83s)  persistentvolume-controller  no persistent volumes available for this claim and no storage class is set
Mounted By:  web-pv-static-subpath
```

tworzymy zatem nowy PersistentVolume `demo-pv-subpath`, aktualizujemy PersistentVolumeClaim `demo-pv-subpath` tak aby korzystaÅ‚ z `demo-pv-subpath`.
NastÄ™pnie w InitContainer naszego pod stworzymy nowy plik `index.html` ktÃ³ry nastÄ™pnie zamontujemy jako `subpath` do wlaÅ›ciwego kontenera
```
apiVersion: v1
kind: Pod
metadata:
  name: web-pv-static-subpath
spec:
  initContainers:
    - image: ubuntu:latest
      name: setup
      volumeMounts:
        - name: workdir
          mountPath: /tempdir
      command: ["/bin/sh", "-c"]
      args:
        - echo "hello from subpath!!!" > /tempdir/index.html;
  containers:
    - image: nginx:1.17.6
      name: web
      ports:
        - containerPort: 80
      volumeMounts:
        - name: workdir
          mountPath: /usr/share/nginx/html/index.html
          subPath: index.html
  volumes:
    - name: workdir
      persistentVolumeClaim:
        claimName: demo-pvc-subpath
```
Dodajemy konfiguracjÄ™ do klastra, widzimy Å¼e nasz PVC zostaÅ‚ podÅ‚Ä…czony do PV.
```
> kubectl apply -f .\pv-pvc-static-subpath.yaml
persistentvolume/demo-pv-subpath created
persistentvolumeclaim/demo-pvc-subpath configured
pod/web-pv-static-subpath created

> kubectl get pvc
NAME               STATUS   VOLUME            CAPACITY   ACCESS MODES   STORAGECLASS   AGE
demo-pvc-subpath   Bound    demo-pv-subpath   10Mi       RWO                           5m34s
```
Sprawdzamy czy nasz `index.html` zostaÅ‚ odpowiedni podmontowany jako subpath
```
> kubectl port-forward web-pv-static-subpath 8080:80
> curl localhost:8080
hello from subpath!!!
```
---

### PersistenVolumeProvisioner aby zcacheowaÄ‡ repozytorium git za pomocÄ… InitContainer

Zaczynamy od stworzenia `storageClass` ktÃ³ra zapewni nam dostÄ™p do defaultowego provisionera dostÄ™pnego w k8s docker for windows
```
> kubectl get sc 
NAME                 PROVISIONER          AGE
hostpath (default)   docker.io/hostpath   6d17h
```
Template naszego storage class:
```
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: demo-sc
provisioner: docker.io/hostpath
reclaimPolicy: Retain
allowVolumeExpansion: true
```
nastÄ™pnie tworzymy nowy PVC ktÃ³ry bÄ™dzie z niej korzystaÅ‚
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: demo-pvc-sc
spec:
  storageClassName: "demo-sc"
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 0.5Mi
```
Na koniec modyfikujemy definicje naszego pod z pierwszego zadania, tak aby korzystaÅ‚ z volume `demo-pvc-sc`.
NastÄ™pnie Dodajemy konfiguracjÄ™ do klastra
```
> kubectl apply -f .\sc-pvc-dynamic.yaml
storageclass.storage.k8s.io/demo-sc created
persistentvolumeclaim/demo-pvc-sc created
pod/web-pv-dynamic created
```
Widzimy, Å¼e nasz PVC zostaÅ‚ z-boundowany do PV, w odrÃ³znieniu od statycznych PVC widzimy Å¼e przypisane mu zostaÅ‚ jedynie zarequestowane 0.5Mi
```
> kubectl get pvc
NAME               STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
demo-pvc           Bound    demo-pv                                    10Mi       RWO                           165m
demo-pvc-sc        Bound    pvc-050e4fb5-4043-11ea-b471-00155d006a01   512Ki      RWO            demo-sc        20s
demo-pvc-subpath   Bound    demo-pv-subpath                            10Mi       RWO                           62m
```
Potwierdzamy, Å¼e POD zadziaÅ‚aÅ‚ zgodnie z oczekiwaniami
```
> kubectl port-forward web-pv-dynamic 8080:80

> curl localhost:8080
<h1>Ahoj! ğŸš¢ğŸ“¦ğŸ´â€â˜ ï¸</h1
```
---

### PersistenVolumeProvisioner aby dostarczyÄ‡ plik konfiguracyjny za pomocÄ… subpath

Tworzymy PVC ktÃ³ry uÅ¼yje ponownie storageClass `demo-sc` stworzonej w poprzednim zadaniu
```
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: demo-pvc-sc-subpath
spec:
  storageClassName: "demo-sc"
  accessModes:
  - ReadWriteOnce
  resources:
    requests:
      storage: 0.5Mi
```
delikatnie modyfikujemy POD uÅ¼yty powyÅ¼ej dla przykÅ‚adu ze statycznym PV-PVC Å¼eby korzystaÅ‚ z volume `demo-pvc-sc-subpath`
i wgrywamy konfiguracjÄ… na klaster
```
> kubectl apply -f .\sc-pvc-dynamic-subpath.yaml
persistentvolumeclaim/demo-pvc-sc-subpath created
pod/web-pv-dynamic-subpath created
```
widzimy, Å¼e nasz PVC zostaÅ‚ bez problemu przypiÄ™ty do juÅ¼ wykorzystywanego storageClass
```
> kubectl get pvc
NAME                  STATUS   VOLUME                                     CAPACITY   ACCESS MODES   STORAGECLASS   AGE
demo-pvc              Bound    demo-pv                                    10Mi       RWO                           3h14m
demo-pvc-sc           Bound    pvc-050e4fb5-4043-11ea-b471-00155d006a01   512Ki      RWO            demo-sc        29m
demo-pvc-sc-subpath   Bound    pvc-1544348f-4047-11ea-b471-00155d006a01   512Ki      RWO            demo-sc        19s
demo-pvc-subpath      Bound    demo-pv-subpath                            10Mi       RWO                           91m
```
Potwierdzamy, Å¼e nasz index.html zostaÅ‚ dodany za pomocÄ… subpath jest hostowany przez nginx
```
> kubectl port-forward web-pv-dynamic-subpath 8080:80

> curl localhost:8080
hello from subpath!!!
```

---

### Odpowiedz sobie na pytanie kiedy moÅ¼e Ci siÄ™ przydaÄ‡ Twoja wÅ‚asna klasa StorageClass?